\documentclass[sigconf, 11pt]{acmart}
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{} 
\pagestyle{plain} 

\begin{document}
\title{An Analysis in Situational Factors on Determining Credit Default Rates}
\subtitle{Authors}

\author{Spencer Hanson}
\affiliation{}
\email{spha0827@colorado.edu}

\author{Ben Miller}
\affiliation{}
\email{@colorado.edu}

\author{Maxim Moghadam}
\affiliation{}
\email{mamo5089@colorado.edu}

\author{William Brickowski}
\affiliation{}
\email{@colorado.edu}

\begin{abstract}
Analyzing local economic conditions to predict default rates.
\end{abstract}

\maketitle



\section{Introduction}
Credit scores are an important factor in many financial decisions, especially in determining whether or not to give out a loan. Financial institutions use credit scores to determine eligibility, taking into account income, loan principle, loan purpose, number of borrowers, and postal code among other factors. By integrating macroeconomic and crime data with the Freddie Mac data set containing the more commonly used attributes we aim to develop a more accurate credit score that is better at predicting a loan default.

\section{Previous Work}
Since its inception in the 1950s, credit scoring has been one of the fastest growing fields in statistical analytics. Because of this there is an abundance of work in the formulation of statistical credit scoring models, and all of this work has been published relatively recently. We look specifically to three types of literature in the field: Classic credit analysis, models for credit default, and statistical shortcomings of credit analysis.

\subsection{Classic credit analysis}

\subsection{Models for Credit Default}
The focus of this project will be on creating a model that can accurately predict credit default rates. Much work has been done in forming statistical models to achieve this. These models include Logistic Regression, K-Nearest Neighbors, K-fold Cross Validation, and Random Forest. We look to an archive of statistical papers written on the subject, specifically examples of the use of logit and probit regressions,\footnote{Qingfen Zhang. 2015. Modeling the Probability of Mortgage Default via Logistic Regression and Survival Analysis. (2015). http://digitalcommons.uri.edu/cgi/viewcontent.cgi?article=1543\&context=theses} but also more complicated dynamic models\footnote{John Y. Cambell and Joao F. Cocco. 2011. A Model of Mortgage Default. (2011). https://scholar.harvard.edu/files/campbell/files/mortdefault13022014.pdf} and instances of machine learning models such as random forests.\footnote{Grace Deng. 2011. Analyzing the Risk of Mortgage Default. (2011). https://www.stat.berkeley.edu/\~aldous/Research/Ugrad/Grace\_Deng\_thesis.pdf}

\subsection{Statistical Shortcomings of Credit analysis}
Lending institutions employ two measures of scoring credit, namely, bureau scores and application scores. The former measure focuses solely on past credit history, while the latter includes other weighting factors such as age and location along with credit history. The issues with bureau credit scores stem primarily from omitted variable bias,\footnote{Robert B. Avery, Raphael W. Bostic, Paul S. Calem, and Glenn B. Canner. 2000. Credit Scoring: Statistical Issues and Evidence from Credit-Bureau Files. Real Estate Economics 28, 3 (2000), 523â€“547. DOI:http://dx.doi.org/10.1111/1540-6229.00811} where local economic conditions and business cycles are not taken into account in the predictive default model. Additional problems with credit analysis can be attributed to data quality issues.\footnote{Robert B. Avery, Paul S. Calem, and Glenn B. Canner. 2011. Credit Report Accuracy and Access to Credit. (2011). https://www.federalreserve.gov/pubs/bulletin/2004/summer04\_credit.pdf}

\section{Data}

\subsection{Loan-Level Mortgages}
Freddie Mac began reporting loan-level credit performance data at the direction of its regulator, the Federal Housing Finance Agency (FHFA) with the stated purpose of increasing transparency, which "helps investors build more accurate credit performance models in support of potential risk-sharing initiatives highlighted in FHFA's Conservatorship Scorecard." We have found a single family loan-level dataset that includes loan-level origination, loan performance, and actual loss data on a proportion of single family mortgages acquired by Freddie Mac. The data contains mortgages from January 1, 1999 to December 31, 2016. 
\subsection{Macroeconomic Data}
The Bureau of Economic Analysis has economic profiles available on every local area in the country. This includes metropolitan and micropolitan areas in every state with measures of income, employment, retirement, and population. 

\section{Proposed Work}
\subsection{Preprocessing}
We will need to convert our data into compatible forms and then merge them together. This will include creating matching null values for unreported or incorrect observations and also matching monthly performance data to originated loan data, and also matching economic data geographically to the location the loan was made.
\subsection{Design}
\subsection{Evaluation}
A loan that became at least 60 days delinquent at some point in its life, will be used as our standard for "default".

\section{Models for prediction of default}
\subsection{Logit and Probit Regression}
Regression will be our baseline approach for analyzing default rates. Classic linear regression relates some independent variable $X$ to the behavior of an ostensibly dependent variable $Y$. In our project, $X$ will represent a matrix of observations while $Y$ will be a categorical or probabilistic vector representing the event of defaulting on a loan, in this event $Y_i$ will take on the value $0$ or $1$ (categorical) or a value between $0$ and $1$ (probabilistic).
\subsection{Survival Analysis}
Survival analysis is a branch in statistics that focuses on analyzing the expected time that passes before some event occurs, such as an earthquake or a death from some disease. Our analysis will focus on predicting the probability an individual has defaulted on their loan at time period $\tau$ defined as $F(\tau) = P(T<\tau)$. From this we can define a survival function that represents the probability that an individual has not defaulted on their loan at time $\tau$: $S(\tau) = P(T>\tau) = 1-F(\tau)$. If we set t equal to the maturity date of the loan this is equivalent to the probability an individual will never default on their loan.
\subsection{Linear Discriminant Analysis}
LDA is a statistical method used in machine learning to find a linear combination of features that characterizes two or more events. We will denote these two events as an individual defaulting ($y=1$)or not defaulting ($y=0$) on a loan. LDA approaches this problem by assuming the conditional distributions the functional density ($P(x|y=0)$ and $P(x|y=1)$) are normal. Hypothesis testing is then performed on the log of the likelihood ratios to determine the probability of default.
\subsection{Random Forest}
Decision trees are a popular statistical method for in the field of machine learning, but they contain drawbacks in application because of their invariance under scaling, along with other reasons. Random forests applies the technique of bootstrapping to decision trees, selecting random samples with replacement of the training set and fitting trees to each respective sample. This is the most complicated statistical method mentioned and its inclusion will be conditional on the amount of time it takes to form models for the other methods.



\section{Tools}
Python, Pandas, Numpy, SciKit-Learn
\section{Milestones}
Load sample data into python \\
Download and have all data available \\
Clean and standardize all datasets \\
Start merge between the datasets \\
Perform an analysis of attributes, e.g. correlation measures\\
Perform basic linear regressions on data \\
Perform logistic/probabilistic regressions on data \\
Apply machine learning methods to data \\
Document and process results \\

\bibliographystyle{ACM-Reference-Format}

\end{document}
